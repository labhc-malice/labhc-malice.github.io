---
layout: reading
key: 2021_Mclenny_L_w-aaai-mlps_sapinns
title: Self-Adaptive PINNs using a Soft Attention Mechanism
description: Additionnal learning of multiplicative soft attention masks to weight each training point individually
giscus_comments: false
date: 2023-06-05
tags: reading
bibliography: 2023-06-05-sapinns.bib
authors:
  - name: RÃ©mi Emonet
    url: "https://home.heeere.com/"
    affiliations:
      name: LabHC, UJM
  - name: Jordan Frecon
    url: "https://jordan-frecon.com/"
    affiliations:
      name: LabHC, UJM
  - name: Amaury Habrard
    url: "https://perso.univ-st-etienne.fr/habrarda/"
    affiliations:
      name: LabHC, UJM
---


**Context:** PINNs may not fit the residual/boundary/initial conditions in "rapidly varying regions" depending on the PDE.

**Proposed solution:** The neural network learns which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, like the one used in computer vision. Each data point is associated with its self-adaptation weight.
- Cons:
	- 3 hyperparameters per training instance + one learning rate
	- Max? A bit heuristic
	- Need to always use the same colocation points


**Proposed solution (SGD):** extension to handle varying colocation points. The basic idea is to use a spatial-temporal predictor of the value of self-adaptive weights for the newly sampled points. Resort to a Gaussian process.


**Other previously proposed solutions:** 
- Non-adaptive Weighting <d-cite key="2020_Zhao_C_j-ccp_saccheapinn"></d-cite>: put premium in the loss for the initial 
- Learning Rate Annealing <d-cite key="2021_Wang_S_j-sc_umgfppinn"></d-cite>: the weights are learning rate coefficients that change for each epoch according to statistics calculated using the data of the back-propagation
- Neural Tangent Kernel Weighting <d-cite key="2022_Zang_S_j-cp_wwpft"></d-cite>: based on the evolution of the eigenvalues
- Mimimax Weighting: update the weights during training using gradient descent for the network weights, and gradient ascent for the loss weights, seeking 
to find a saddle point in weight space.



