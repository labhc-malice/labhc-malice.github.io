---
layout: reading
key: 2024_Liu_N_p-icml_clawno
title: Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws
description: Using the theory of differential forms to hard-code divergence-free constraints in neural operators
tldr: The paper introduces ClawNO, a Neural Operator that incorporates a divergence-free constraint directly into its architecture, ensuring physically consistent learning.
insider: ""
giscus_comments: false
date: 2025-02-10
tags: reading
authors:
  - name: Abdel-Rahim Mezidi
  - name: Erick Gomez
    affiliations:
      name: LabHC, UJM
  - name: Volodimir Mitarchuk

      
---

**Context:**
A **continuity equation** states that the rate of change of a conserved quantity within a volume is equal to the net flux of that quantity through its boundary. In fluid mechanics, this is expressed as:

$$
\frac{\partial \rho}{\partial t} + \nabla \cdot ( \rho \mathbf{u} ) = 0,
$$

where $$\rho$$ is the density, and $$\mathbf{u}$$ is the velocity field. Incompressibility implies constant $$\rho$, thus we have:

$$
\nabla \cdot \mathbf{u} = 0.
$$

This condition $$\nabla \cdot \mathbf{u} = 0$$ is often referred to as the **divergence-free** constraint.

Standard **Neural Operators (NOs)** learn mappings between function spaces purely from data, without explicitly enforcing conservation laws. As a result, they **lack robustness** in small-data regimes and may violate physical laws.

**Proposed solution:**
The paper proposes **ClawNO**, a Neural Operator that **automatically enforces conservation laws** within its architecture, ensuring that the output of the model is **divergence-free**.

Unlike **Physics-Informed Neural Operators (PINO)** soft constraints, which are imposed through additional loss functions that do not guarantee their respect, ClawNO hard constraints ensure their enforcement, leading to **more stable and data-efficient learning**.

<center>
<img src="https://raw.githubusercontent.com/ningliu-iga/clawNO/refs/heads/main/assets/clawNO_architecture.png" style="max-width: 80%;height: auto;border-radius: 10px">
</center>
	
Instead of directly outputting the solution $$u$$ after the projection layer $$Q$$, ClawNOâ€™s projection layer produces a latent vector used to construct a **skew-symmetric matrix** $$\mu$$.
The final output $$u$$ is obtained by applying a **weighted linear combination operator** $$D$$, which **approximates the row-wise divergence operator**.
Because the divergence of a skew-symmetric matrix is **theoretically divergence-free**, the model **enforces this constraint inherently**.

For **non-periodic domains**, ClawNO leverages the **Fourier Continuation (FC) technique** to extend the output into a periodic function.


- Pros:
  - Improves **robustness** in **small-data regimes**.
  - Is **model-agnostic**: this approach could be applied to any neural operator architecture.

- Cons:
  - May be **computationally expensive** due to the additional computations.
  - The model was compared to standard FNO but not to PINO with conservation laws constraints, which could have provided a more fair comparison.
  - Because of the approximations made by the divergence operator, the model may not be able to enforce an exact divergence-free field.

